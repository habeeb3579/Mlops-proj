{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pickle\n",
    "from typing import Union, Tuple, List, Dict\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKING_SERVER_HOST = \"127.0.0.1\" # fill in with the public IP\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = \"/home/habeeb/Mlops-proj/04-deployment/streaming/lambda_functions/model.pkl\"\n",
    "with open(mdl_path, \"rb\") as f:\n",
    "    model2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model: nyc-taxi-regressor-weighted-main9 @ production\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nyc-taxi-regressor-weighted-main9\"\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}@production\")\n",
    "print(f\"‚úÖ Loaded model: {model_name} @ production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.loaded_model:\n",
       "  artifact_path: model\n",
       "  flavor: mlflow.sklearn\n",
       "  run_id: da62ead94d284c08aca4594e17885e42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuids(n):\n",
    "    return [str(uuid.uuid4()) for i in range(n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data: Union[pd.DataFrame, List[Dict], Dict]) -> Tuple[pd.DataFrame, Union[np.ndarray, None]]:\n",
    "    \"\"\"\n",
    "    Prepare taxi trip data for model input.\n",
    "    \n",
    "    Supports:\n",
    "      - DataFrame input\n",
    "      - List of dictionaries\n",
    "      - Single dictionary input\n",
    "    \n",
    "    Handles both lpep and tpep datetime columns.\n",
    "    \n",
    "    Args:\n",
    "        data: Input data\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (processed DataFrame, target if available else None)\n",
    "    \"\"\"\n",
    "    # Normalize input to DataFrame\n",
    "    if isinstance(data, dict):\n",
    "        df = pd.DataFrame([data])\n",
    "    elif isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df = data.copy()\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a DataFrame, list of dicts, or single dict.\")\n",
    "\n",
    "    # Determine pickup/dropoff column names\n",
    "    if 'lpep_pickup_datetime' in df.columns and 'lpep_dropoff_datetime' in df.columns:\n",
    "        pickup_col, dropoff_col = 'lpep_pickup_datetime', 'lpep_dropoff_datetime'\n",
    "    elif 'tpep_pickup_datetime' in df.columns and 'tpep_dropoff_datetime' in df.columns:\n",
    "        pickup_col, dropoff_col = 'tpep_pickup_datetime', 'tpep_dropoff_datetime'\n",
    "    else:\n",
    "        pickup_col, dropoff_col = None, None\n",
    "\n",
    "    # Handle datetime conversion and duration calculation\n",
    "    if pickup_col and dropoff_col:\n",
    "        if not np.issubdtype(df[pickup_col].dtype, np.datetime64):\n",
    "            df[pickup_col] = pd.to_datetime(df[pickup_col])\n",
    "        if not np.issubdtype(df[dropoff_col].dtype, np.datetime64):\n",
    "            df[dropoff_col] = pd.to_datetime(df[dropoff_col])\n",
    "\n",
    "        df[\"duration\"] = (df[dropoff_col] - df[pickup_col]).dt.total_seconds() / 60\n",
    "        df = df[(df[\"duration\"] >= 1) & (df[\"duration\"] <= 60)]\n",
    "    else:\n",
    "        df[\"duration\"] = None\n",
    "\n",
    "    # Convert categorical columns to string if present\n",
    "    for col in [\"PULocationID\", \"DOLocationID\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    # Create combined PU_DO feature\n",
    "    if \"PULocationID\" in df.columns and \"DOLocationID\" in df.columns:\n",
    "        df[\"PU_DO\"] = df[\"PULocationID\"] + \"_\" + df[\"DOLocationID\"]\n",
    "\n",
    "    df['ride_id'] = generate_uuids(len(df))\n",
    "\n",
    "    # Return target if fully computed\n",
    "    target = df[\"duration\"].values if df[\"duration\"].notna().all() else None\n",
    "    return df, target\n",
    "\n",
    "def predict(features):\n",
    "    preds = model.predict(features)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"lpep_pickup_datetime\": \"2021-02-01T08:00:00\",\n",
    "        \"lpep_dropoff_datetime\": \"2021-02-01T08:15:00\",\n",
    "        \"PULocationID\": 132,\n",
    "        \"DOLocationID\": 138,\n",
    "        'trip_distance': 50\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs, targ = prepare_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>duration</th>\n",
       "      <th>PU_DO</th>\n",
       "      <th>ride_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-01 08:00:00</td>\n",
       "      <td>2021-02-01 08:15:00</td>\n",
       "      <td>132</td>\n",
       "      <td>138</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>132_138</td>\n",
       "      <td>fb425df3-4955-4f8e-9ae6-67b15988b43c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime PULocationID DOLocationID  \\\n",
       "0  2021-02-01 08:00:00   2021-02-01 08:15:00          132          138   \n",
       "\n",
       "   trip_distance  duration    PU_DO                               ride_id  \n",
       "0             50      15.0  132_138  fb425df3-4955-4f8e-9ae6-67b15988b43c  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.710823], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.710823], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: mlflow models [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Deploy MLflow models locally.\n",
      "\n",
      "  To deploy a model associated with a run on a tracking server, set the\n",
      "  MLFLOW_TRACKING_URI environment variable to the URL of the desired server.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  build-docker             Builds a Docker image whose default entrypoint...\n",
      "  generate-dockerfile      Generates a directory with Dockerfile whose...\n",
      "  predict                  Generate predictions in json format using a...\n",
      "  prepare-env              Performs any preparation necessary to predict...\n",
      "  serve                    Serve a model saved with MLflow by launching a...\n",
      "  update-pip-requirements  Add or remove requirements from a model's...\n"
     ]
    }
   ],
   "source": [
    "!mlflow models --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Union, Any\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "class MinimalDataFrame:\n",
    "    \"\"\"Enhanced DataFrame-like class for ColumnTransformer compatibility\"\"\"\n",
    "    \n",
    "    def __init__(self, data: Dict[str, List]):\n",
    "        self.data = data\n",
    "        self.columns = list(data.keys())\n",
    "        self.shape = (len(next(iter(data.values()))) if data else 0, len(data))\n",
    "        self.index = list(range(self.shape[0]))\n",
    "        self._setup_dtypes()\n",
    "    \n",
    "    def _setup_dtypes(self):\n",
    "        \"\"\"Determine data types for each column\"\"\"\n",
    "        self.dtypes = {}\n",
    "        for col, values in self.data.items():\n",
    "            if not values:\n",
    "                self.dtypes[col] = 'object'\n",
    "                continue\n",
    "            \n",
    "            # Check if all values are numeric\n",
    "            sample_val = next((v for v in values if v is not None), None)\n",
    "            if isinstance(sample_val, (int, float)):\n",
    "                self.dtypes[col] = 'float64'\n",
    "            else:\n",
    "                self.dtypes[col] = 'object'\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.data[key]\n",
    "        elif isinstance(key, list):\n",
    "            return MinimalDataFrame({k: self.data[k] for k in key if k in self.data})\n",
    "        elif isinstance(key, tuple) and len(key) == 2:\n",
    "            # Handle df[rows, cols] indexing\n",
    "            rows, cols = key\n",
    "            if isinstance(cols, str):\n",
    "                cols = [cols]\n",
    "            elif cols is None:\n",
    "                cols = self.columns\n",
    "            \n",
    "            # Get row indices\n",
    "            if isinstance(rows, slice):\n",
    "                row_indices = list(range(*rows.indices(self.shape[0])))\n",
    "            elif hasattr(rows, '__iter__'):\n",
    "                row_indices = list(rows)\n",
    "            else:\n",
    "                row_indices = [rows]\n",
    "            \n",
    "            # Extract data\n",
    "            result_data = {}\n",
    "            for col in cols:\n",
    "                if col in self.data:\n",
    "                    result_data[col] = [self.data[col][i] for i in row_indices]\n",
    "            \n",
    "            return MinimalDataFrame(result_data)\n",
    "        else:\n",
    "            raise KeyError(f\"Unsupported key type: {type(key)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.shape[0]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over column names\"\"\"\n",
    "        return iter(self.columns)\n",
    "    \n",
    "    def __array__(self):\n",
    "        \"\"\"Convert to numpy array for sklearn compatibility - DO NOT USE\"\"\"\n",
    "        # This method should NOT be called by ColumnTransformer\n",
    "        # ColumnTransformer should use column selection instead\n",
    "        raise NotImplementedError(\"ColumnTransformer should use column selection, not __array__\")\n",
    "    \n",
    "    @property\n",
    "    def values(self):\n",
    "        \"\"\"Return values as numpy array - only when explicitly requested\"\"\"\n",
    "        if not self.data:\n",
    "            return np.array([]).reshape(0, 0)\n",
    "        \n",
    "        # Convert all data to appropriate types\n",
    "        arrays = []\n",
    "        for col in self.columns:\n",
    "            col_data = self.data[col]\n",
    "            try:\n",
    "                # Try to convert to numeric\n",
    "                numeric_data = [float(x) if x is not None else np.nan for x in col_data]\n",
    "                arrays.append(numeric_data)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion fails, keep as object\n",
    "                arrays.append(col_data)\n",
    "        \n",
    "        return np.column_stack(arrays) if arrays else np.array([]).reshape(self.shape)\n",
    "    \n",
    "    def iloc(self, row_indexer, col_indexer=None):\n",
    "        if col_indexer is None:\n",
    "            col_indexer = slice(None)\n",
    "        \n",
    "        # Handle column selection\n",
    "        if isinstance(col_indexer, slice):\n",
    "            selected_cols = self.columns[col_indexer]\n",
    "        elif isinstance(col_indexer, (list, tuple)):\n",
    "            selected_cols = [self.columns[i] if isinstance(i, int) else i for i in col_indexer]\n",
    "        elif isinstance(col_indexer, int):\n",
    "            selected_cols = [self.columns[col_indexer]]\n",
    "        else:\n",
    "            selected_cols = self.columns\n",
    "        \n",
    "        # Handle row selection\n",
    "        if isinstance(row_indexer, slice):\n",
    "            row_indices = list(range(*row_indexer.indices(self.shape[0])))\n",
    "        elif hasattr(row_indexer, '__iter__') and not isinstance(row_indexer, str):\n",
    "            row_indices = list(row_indexer)\n",
    "        elif isinstance(row_indexer, int):\n",
    "            row_indices = [row_indexer]\n",
    "        else:\n",
    "            row_indices = list(range(self.shape[0]))\n",
    "        \n",
    "        # Extract data\n",
    "        result_data = {}\n",
    "        for col in selected_cols:\n",
    "            if col in self.data:\n",
    "                result_data[col] = [self.data[col][i] for i in row_indices]\n",
    "        \n",
    "        return MinimalDataFrame(result_data)\n",
    "    \n",
    "    def loc(self, row_indexer, col_indexer=None):\n",
    "        \"\"\"Basic loc implementation\"\"\"\n",
    "        return self.iloc(row_indexer, col_indexer)\n",
    "    \n",
    "    def copy(self):\n",
    "        return MinimalDataFrame({k: v.copy() for k, v in self.data.items()})\n",
    "    \n",
    "    def select_dtypes(self, include=None, exclude=None):\n",
    "        \"\"\"Select columns by dtype\"\"\"\n",
    "        if include is None and exclude is None:\n",
    "            return self.copy()\n",
    "        \n",
    "        selected_cols = []\n",
    "        for col in self.columns:\n",
    "            dtype = self.dtypes.get(col, 'object')\n",
    "            \n",
    "            include_col = True\n",
    "            if include is not None:\n",
    "                if isinstance(include, str):\n",
    "                    include = [include]\n",
    "                include_col = any(inc in dtype for inc in include)\n",
    "            \n",
    "            if exclude is not None and include_col:\n",
    "                if isinstance(exclude, str):\n",
    "                    exclude = [exclude]\n",
    "                include_col = not any(exc in dtype for exc in exclude)\n",
    "            \n",
    "            if include_col:\n",
    "                selected_cols.append(col)\n",
    "        \n",
    "        return MinimalDataFrame({k: self.data[k] for k in selected_cols})\n",
    "    \n",
    "    def drop(self, columns=None, axis=1):\n",
    "        \"\"\"Drop columns\"\"\"\n",
    "        if axis != 1:\n",
    "            raise NotImplementedError(\"Only column dropping supported\")\n",
    "        \n",
    "        if isinstance(columns, str):\n",
    "            columns = [columns]\n",
    "        \n",
    "        remaining_cols = [col for col in self.columns if col not in columns]\n",
    "        return MinimalDataFrame({k: self.data[k] for k in remaining_cols})\n",
    "    \n",
    "    def reset_index(self, drop=True):\n",
    "        \"\"\"Reset index (no-op for this implementation)\"\"\"\n",
    "        return self.copy()\n",
    "\n",
    "def features_to_minimal_df(features: List[Dict]) -> MinimalDataFrame:\n",
    "    \"\"\"Convert feature list to MinimalDataFrame\"\"\"\n",
    "    if not features:\n",
    "        return MinimalDataFrame({})\n",
    "    \n",
    "    # Get all unique keys\n",
    "    all_keys = set()\n",
    "    for feature in features:\n",
    "        all_keys.update(feature.keys())\n",
    "    \n",
    "    # Create column-oriented dictionary\n",
    "    df_dict = {}\n",
    "    for key in all_keys:\n",
    "        df_dict[key] = [feature.get(key) for feature in features]\n",
    "    \n",
    "    return MinimalDataFrame(df_dict)\n",
    "\n",
    "def predict2(features):\n",
    "    \"\"\"Simple predict function - let ColumnTransformer handle column selection\"\"\"\n",
    "    # Handle single dictionary input\n",
    "    if isinstance(features, dict):\n",
    "        features = [features]\n",
    "    \n",
    "    if not features:\n",
    "        return []\n",
    "    \n",
    "    # Convert to MinimalDataFrame with all available columns\n",
    "    df_like = features_to_minimal_df(features)\n",
    "    \n",
    "    # Let the ColumnTransformer select only the columns it needs\n",
    "    preds = model.predict(df_like)\n",
    "    return preds\n",
    "\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "def parse_datetime_fast(dt_str: str) -> Optional[datetime]:\n",
    "    \"\"\"Optimized datetime parsing with minimal try/except overhead\"\"\"\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle ISO format first (most common)\n",
    "        if \"T\" in dt_str:\n",
    "            return datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        else:\n",
    "            # Handle space-separated format\n",
    "            return datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except Exception:\n",
    "        print(f\"‚ö†Ô∏è Could not parse datetime: {dt_str}\")\n",
    "        return None\n",
    "\n",
    "def calculate_duration(pickup_str: str, dropoff_str: str) -> Optional[float]:\n",
    "    \"\"\"Calculate duration in minutes with validation\"\"\"\n",
    "    pickup_dt = parse_datetime_fast(pickup_str)\n",
    "    dropoff_dt = parse_datetime_fast(dropoff_str)\n",
    "    \n",
    "    if not pickup_dt or not dropoff_dt:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        duration = (dropoff_dt - pickup_dt).total_seconds() / 60.0\n",
    "        # Filter outliers (1-60 minutes)\n",
    "        return duration if 1 <= duration <= 60 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def prepare_single_feature(row: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Process a single row efficiently\"\"\"\n",
    "    # Extract datetime fields\n",
    "    pickup = row.get(\"lpep_pickup_datetime\") or row.get(\"tpep_pickup_datetime\")\n",
    "    dropoff = row.get(\"lpep_dropoff_datetime\") or row.get(\"tpep_dropoff_datetime\")\n",
    "    \n",
    "    if not pickup or not dropoff:\n",
    "        return None\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = calculate_duration(pickup, dropoff)\n",
    "    if duration is None:\n",
    "        return None\n",
    "    \n",
    "    # Create feature dict with minimal copying\n",
    "    feature = {\n",
    "        \"duration\": duration,\n",
    "        \"ride_id\": str(uuid.uuid4())\n",
    "    }\n",
    "    \n",
    "    # Add location IDs as strings\n",
    "    if \"PULocationID\" in row:\n",
    "        feature[\"PULocationID\"] = str(row[\"PULocationID\"])\n",
    "    if \"DOLocationID\" in row:\n",
    "        feature[\"DOLocationID\"] = str(row[\"DOLocationID\"])\n",
    "    \n",
    "    # Create PU_DO combination if both locations exist\n",
    "    if \"PULocationID\" in feature and \"DOLocationID\" in feature:\n",
    "        feature[\"PU_DO\"] = f\"{feature['PULocationID']}_{feature['DOLocationID']}\"\n",
    "    \n",
    "    # Copy other relevant fields without deep copying\n",
    "    for key in [\"passenger_count\", \"trip_distance\", \"fare_amount\", \"total_amount\"]:\n",
    "        if key in row:\n",
    "            feature[key] = row[key]\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def prepare_features_batch(data: Union[Dict, List[Dict]]) -> tuple[List[Dict], int]:\n",
    "    \"\"\"Optimized batch feature preparation\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "    \n",
    "    features = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    for row in data:\n",
    "        try:\n",
    "            feature = prepare_single_feature(row)\n",
    "            if feature:\n",
    "                features.append(feature)\n",
    "                processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing row: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return features, processed_count\n",
    "\n",
    "# Usage example:\n",
    "def predict_from_raw_data(data):\n",
    "    \"\"\"Complete pipeline: raw data -> features -> prediction\"\"\"\n",
    "    # Prepare features using your existing function\n",
    "    features, count = prepare_features_batch(data)\n",
    "    \n",
    "    if not features:\n",
    "        return []\n",
    "    \n",
    "    # Make prediction\n",
    "    return predict2(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 6 features, but ColumnTransformer is expecting 22 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_from_raw_data(data)\n",
      "Cell \u001b[0;32mIn[21], line 310\u001b[0m, in \u001b[0;36mpredict_from_raw_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predict2(features)\n",
      "Cell \u001b[0;32mIn[21], line 205\u001b[0m, in \u001b[0;36mpredict2\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    202\u001b[0m df_like \u001b[38;5;241m=\u001b[39m features_to_minimal_df(features)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Let the ColumnTransformer select only the columns it needs\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(df_like)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:812\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;241m:=\u001b[39m _get_dependencies_schema_from_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_meta):\n\u001b[1;32m    811\u001b[0m     context\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mschema)\n\u001b[0;32m--> 812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(data, params)\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:862\u001b[0m, in \u001b[0;36mPyFuncModel._predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    860\u001b[0m params_arg \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params_arg \u001b[38;5;129;01mand\u001b[39;00m params_arg\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mVAR_KEYWORD:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    864\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/mlflow/sklearn/__init__.py:541\u001b[0m, in \u001b[0;36m_SklearnModelWrapper.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    530\u001b[0m     data,\n\u001b[1;32m    531\u001b[0m     params: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    532\u001b[0m ):\n\u001b[1;32m    533\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m        data: Model input data.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m        Model predictions.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msklearn_model\u001b[38;5;241m.\u001b[39mpredict(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/pipeline.py:600\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:1069\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n\u001b[1;32m   1072\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 6 features, but ColumnTransformer is expecting 22 features as input."
     ]
    }
   ],
   "source": [
    "predict_from_raw_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'duration': 15.0,\n",
       "   'ride_id': '47acc44a-4e29-4c86-b74f-78357d294eff',\n",
       "   'PULocationID': '132',\n",
       "   'DOLocationID': '138',\n",
       "   'PU_DO': '132_138',\n",
       "   'trip_distance': 50}],\n",
       " 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_features_batch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Quick Model Analysis\n",
      "==================================================\n",
      "‚úÖ Loaded model from: /home/habeeb/Mlops-proj/04-deployment/streaming/lambda_functions/model.pkl\n",
      "üìä Model type: <class 'sklearn.pipeline.Pipeline'>\n",
      "üîó Pipeline detected with steps: ['preprocessor', 'regressor']\n",
      "\n",
      "--- Inspecting step: preprocessor ---\n",
      "  Type: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "  üìã ColumnTransformer transformers:\n",
      "    - cat: Pipeline on columns ['PU_DO']\n",
      "    - num: Pipeline on columns ['trip_distance']\n",
      "  üìä All columns used: ['PU_DO', 'trip_distance']\n",
      "  ‚ú® Feature names in: ['VendorID' 'lpep_pickup_datetime' 'lpep_dropoff_datetime'\n",
      " 'store_and_fwd_flag' 'RatecodeID' 'PULocationID' 'DOLocationID'\n",
      " 'passenger_count' 'trip_distance' 'fare_amount' 'extra' 'mta_tax'\n",
      " 'tip_amount' 'tolls_amount' 'ehail_fee' 'improvement_surcharge'\n",
      " 'total_amount' 'payment_type' 'trip_type' 'congestion_surcharge'\n",
      " 'duration' 'PU_DO']\n",
      "  üî¢ Number of features in: 22\n",
      "  üéØ Feature names out: ['cat__PU_DO_100_168' 'cat__PU_DO_100_180' 'cat__PU_DO_100_190' ...\n",
      " 'cat__PU_DO_9_97' 'cat__PU_DO_9_98' 'num__trip_distance']\n",
      "  üìù _columns: [['PU_DO'], ['trip_distance']]\n",
      "\n",
      "--- Inspecting step: regressor ---\n",
      "  Type: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "  üî¢ Number of features in: 13221\n",
      "‚ú® Found feature_names_in_: ['VendorID' 'lpep_pickup_datetime' 'lpep_dropoff_datetime'\n",
      " 'store_and_fwd_flag' 'RatecodeID' 'PULocationID' 'DOLocationID'\n",
      " 'passenger_count' 'trip_distance' 'fare_amount' 'extra' 'mta_tax'\n",
      " 'tip_amount' 'tolls_amount' 'ehail_fee' 'improvement_surcharge'\n",
      " 'total_amount' 'payment_type' 'trip_type' 'congestion_surcharge'\n",
      " 'duration' 'PU_DO']\n",
      "üî¢ Number of input features: 22\n",
      "\n",
      "==================================================\n",
      "üìã SUMMARY\n",
      "==================================================\n",
      "‚úÖ Found 22 training columns:\n",
      "   1. VendorID\n",
      "   2. lpep_pickup_datetime\n",
      "   3. lpep_dropoff_datetime\n",
      "   4. store_and_fwd_flag\n",
      "   5. RatecodeID\n",
      "   6. PULocationID\n",
      "   7. DOLocationID\n",
      "   8. passenger_count\n",
      "   9. trip_distance\n",
      "  10. fare_amount\n",
      "  11. extra\n",
      "  12. mta_tax\n",
      "  13. tip_amount\n",
      "  14. tolls_amount\n",
      "  15. ehail_fee\n",
      "  16. improvement_surcharge\n",
      "  17. total_amount\n",
      "  18. payment_type\n",
      "  19. trip_type\n",
      "  20. congestion_surcharge\n",
      "  21. duration\n",
      "  22. PU_DO\n",
      "\n",
      "üêç Python list format:\n",
      "EXPECTED_COLUMNS = ['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge', 'duration', 'PU_DO']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import inspect\n",
    "from typing import List, Optional\n",
    "\n",
    "def inspect_pickle_model(model_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Inspect a pickled scikit-learn model to extract column information.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded model from: {model_path}\")\n",
    "        print(f\"üìä Model type: {type(model)}\")\n",
    "        \n",
    "        # Check if it's a Pipeline\n",
    "        if hasattr(model, 'steps'):\n",
    "            print(f\"üîó Pipeline detected with steps: {[step[0] for step in model.steps]}\")\n",
    "            results['model_type'] = 'Pipeline'\n",
    "            results['steps'] = [step[0] for step in model.steps]\n",
    "            \n",
    "            # Look for ColumnTransformer in the pipeline\n",
    "            for step_name, step_transformer in model.steps:\n",
    "                print(f\"\\n--- Inspecting step: {step_name} ---\")\n",
    "                step_info = inspect_transformer(step_transformer, step_name)\n",
    "                results[step_name] = step_info\n",
    "        \n",
    "        # Check if it's a direct ColumnTransformer\n",
    "        elif hasattr(model, 'transformers'):\n",
    "            print(\"üîß Direct ColumnTransformer detected\")\n",
    "            results['model_type'] = 'ColumnTransformer'\n",
    "            transformer_info = inspect_transformer(model, 'main')\n",
    "            results.update(transformer_info)\n",
    "        \n",
    "        # Check for feature names\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            print(f\"‚ú® Found feature_names_in_: {model.feature_names_in_}\")\n",
    "            results['feature_names_in'] = list(model.feature_names_in_)\n",
    "        \n",
    "        # Check for n_features_in_\n",
    "        if hasattr(model, 'n_features_in_'):\n",
    "            print(f\"üî¢ Number of input features: {model.n_features_in_}\")\n",
    "            results['n_features_in'] = model.n_features_in_\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inspecting model: {str(e)}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def inspect_transformer(transformer, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Inspect a specific transformer to extract column information.\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        'type': type(transformer).__name__,\n",
    "        'attributes': {}\n",
    "    }\n",
    "    \n",
    "    print(f\"  Type: {type(transformer)}\")\n",
    "    \n",
    "    # Check for ColumnTransformer specific attributes\n",
    "    if hasattr(transformer, 'transformers'):\n",
    "        print(\"  üìã ColumnTransformer transformers:\")\n",
    "        transformers_info = []\n",
    "        \n",
    "        for trans_name, trans_obj, columns in transformer.transformers:\n",
    "            print(f\"    - {trans_name}: {type(trans_obj).__name__} on columns {columns}\")\n",
    "            transformers_info.append({\n",
    "                'name': trans_name,\n",
    "                'transformer_type': type(trans_obj).__name__,\n",
    "                'columns': columns\n",
    "            })\n",
    "        \n",
    "        info['transformers'] = transformers_info\n",
    "        \n",
    "        # Get all unique columns used\n",
    "        all_columns = []\n",
    "        for _, _, columns in transformer.transformers:\n",
    "            if isinstance(columns, list):\n",
    "                all_columns.extend(columns)\n",
    "            elif columns != 'drop':  # Skip dropped transformers\n",
    "                all_columns.append(columns)\n",
    "        \n",
    "        info['all_columns_used'] = list(set(all_columns))\n",
    "        print(f\"  üìä All columns used: {info['all_columns_used']}\")\n",
    "    \n",
    "    # Check for feature names\n",
    "    if hasattr(transformer, 'feature_names_in_'):\n",
    "        info['feature_names_in'] = list(transformer.feature_names_in_)\n",
    "        print(f\"  ‚ú® Feature names in: {transformer.feature_names_in_}\")\n",
    "    \n",
    "    if hasattr(transformer, 'n_features_in_'):\n",
    "        info['n_features_in'] = transformer.n_features_in_\n",
    "        print(f\"  üî¢ Number of features in: {transformer.n_features_in_}\")\n",
    "    \n",
    "    # Check for get_feature_names_out method\n",
    "    if hasattr(transformer, 'get_feature_names_out'):\n",
    "        try:\n",
    "            if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_ is not None:\n",
    "                feature_names_out = transformer.get_feature_names_out(transformer.feature_names_in_)\n",
    "                info['feature_names_out'] = list(feature_names_out)\n",
    "                print(f\"  üéØ Feature names out: {feature_names_out}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not get feature_names_out: {e}\")\n",
    "    \n",
    "    # Check other common attributes\n",
    "    for attr in ['_columns', 'columns_', 'feature_names_', 'get_feature_names']:\n",
    "        if hasattr(transformer, attr):\n",
    "            try:\n",
    "                value = getattr(transformer, attr)\n",
    "                if callable(value):\n",
    "                    try:\n",
    "                        value = value()\n",
    "                    except:\n",
    "                        value = \"callable (couldn't execute)\"\n",
    "                info['attributes'][attr] = value\n",
    "                print(f\"  üìù {attr}: {value}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Error getting {attr}: {e}\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "def find_training_columns(model_path: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Try to extract the exact column names the model was trained on.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # Strategy 1: Check feature_names_in_ (sklearn 1.0+)\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            return list(model.feature_names_in_)\n",
    "        \n",
    "        # Strategy 2: Check pipeline steps\n",
    "        if hasattr(model, 'steps'):\n",
    "            for step_name, step_transformer in model.steps:\n",
    "                if hasattr(step_transformer, 'feature_names_in_'):\n",
    "                    return list(step_transformer.feature_names_in_)\n",
    "        \n",
    "        # Strategy 3: For ColumnTransformer, reconstruct from transformers\n",
    "        if hasattr(model, 'transformers') or (hasattr(model, 'steps') and \n",
    "            any(hasattr(step[1], 'transformers') for step in model.steps)):\n",
    "            \n",
    "            # Find the ColumnTransformer\n",
    "            ct = model\n",
    "            if hasattr(model, 'steps'):\n",
    "                for step_name, step_transformer in model.steps:\n",
    "                    if hasattr(step_transformer, 'transformers'):\n",
    "                        ct = step_transformer\n",
    "                        break\n",
    "            \n",
    "            if hasattr(ct, 'transformers'):\n",
    "                all_columns = []\n",
    "                for trans_name, trans_obj, columns in ct.transformers:\n",
    "                    if columns != 'drop' and isinstance(columns, (list, tuple)):\n",
    "                        all_columns.extend(columns)\n",
    "                    elif isinstance(columns, str):\n",
    "                        all_columns.append(columns)\n",
    "                \n",
    "                return all_columns\n",
    "        \n",
    "        print(\"‚ùå Could not determine training columns automatically\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error finding training columns: {e}\")\n",
    "        return None\n",
    "\n",
    "def quick_model_check(model_path: str = \"model.pkl\"):\n",
    "    \"\"\"\n",
    "    Quick check to get the essential information about your model.\n",
    "    \"\"\"\n",
    "    print(\"üîç Quick Model Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå Model file not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get full inspection\n",
    "    results = inspect_pickle_model(model_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìã SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Try to find training columns\n",
    "    training_columns = find_training_columns(model_path)\n",
    "    if training_columns:\n",
    "        print(f\"‚úÖ Found {len(training_columns)} training columns:\")\n",
    "        for i, col in enumerate(training_columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\nüêç Python list format:\")\n",
    "        print(f\"EXPECTED_COLUMNS = {training_columns}\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not automatically determine training columns\")\n",
    "        print(\"üí° You may need to check your training script or data\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage functions\n",
    "def inspect_mlflow_model(model_name: str, tracking_uri: str = \"http://127.0.0.1:5000\"):\n",
    "    \"\"\"\n",
    "    Inspect an MLflow model for column information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mlflow\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "        \n",
    "        # Load model\n",
    "        model = mlflow.pyfunc.load_model(f\"models:/{model_name}@production\")\n",
    "        \n",
    "        print(f\"‚úÖ Loaded MLflow model: {model_name}\")\n",
    "        \n",
    "        # Check if it has the underlying sklearn model\n",
    "        if hasattr(model, '_model_impl') and hasattr(model._model_impl, 'python_model'):\n",
    "            sklearn_model = model._model_impl.python_model\n",
    "            print(f\"üìä Underlying model type: {type(sklearn_model)}\")\n",
    "            \n",
    "            # Use the same inspection logic\n",
    "            if hasattr(sklearn_model, 'feature_names_in_'):\n",
    "                print(f\"‚ú® Found feature_names_in_: {sklearn_model.feature_names_in_}\")\n",
    "                return list(sklearn_model.feature_names_in_)\n",
    "        \n",
    "        print(\"‚ùå Could not extract column information from MLflow model\")\n",
    "        return None\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå MLflow not available\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inspecting MLflow model: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the quick check\n",
    "    quick_model_check(mdl_path)\n",
    "    \n",
    "    # If you want to check MLflow model instead:\n",
    "    # inspect_mlflow_model(\"nyc-taxi-regressor-weighted-main9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;PU_DO&#x27;]),\n",
       "                                                 (&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   &#x27;passthrough&#x27;)]),\n",
       "                                                  [&#x27;trip_distance&#x27;])])),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 X...\n",
       "                              importance_type=None,\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.46784237737404744, max_bin=None,\n",
       "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                              max_delta_step=None, max_depth=27,\n",
       "                              max_leaves=None,\n",
       "                              min_child_weight=2.4358687372422665, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, random_state=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;PU_DO&#x27;]),\n",
       "                                                 (&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   &#x27;passthrough&#x27;)]),\n",
       "                                                  [&#x27;trip_distance&#x27;])])),\n",
       "                (&#x27;regressor&#x27;,\n",
       "                 X...\n",
       "                              importance_type=None,\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.46784237737404744, max_bin=None,\n",
       "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                              max_delta_step=None, max_depth=27,\n",
       "                              max_leaves=None,\n",
       "                              min_child_weight=2.4358687372422665, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, random_state=None, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                strategy=&#x27;constant&#x27;)),\n",
       "                                                 (&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;PU_DO&#x27;]),\n",
       "                                (&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                 (&#x27;scaler&#x27;, &#x27;passthrough&#x27;)]),\n",
       "                                 [&#x27;trip_distance&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">cat</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;PU_DO&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(fill_value=&#x27;missing&#x27;, strategy=&#x27;constant&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">num</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;trip_distance&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBRegressor</label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.6387055384703344, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.46784237737404744, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=27, max_leaves=None,\n",
       "             min_child_weight=2.4358687372422665, missing=nan,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "             n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['PU_DO']),\n",
       "                                                 ('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   'passthrough')]),\n",
       "                                                  ['trip_distance'])])),\n",
       "                ('regressor',\n",
       "                 X...\n",
       "                              importance_type=None,\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.46784237737404744, max_bin=None,\n",
       "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                              max_delta_step=None, max_depth=27,\n",
       "                              max_leaves=None,\n",
       "                              min_child_weight=2.4358687372422665, missing=nan,\n",
       "                              monotone_constraints=None, multi_strategy=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, random_state=None, ...))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for model inspection\n",
    "EXPECTED_COLUMNS = None\n",
    "DEFAULT_VALUES = None\n",
    "\n",
    "def extract_model_columns_and_defaults(model_instance):\n",
    "    \"\"\"\n",
    "    Extract expected column names from the trained model and create sensible defaults.\n",
    "    \"\"\"\n",
    "    global EXPECTED_COLUMNS, DEFAULT_VALUES\n",
    "    \n",
    "    if EXPECTED_COLUMNS is not None and DEFAULT_VALUES is not None:\n",
    "        return EXPECTED_COLUMNS, DEFAULT_VALUES\n",
    "    \n",
    "    try:\n",
    "        # Strategy 1: Check feature_names_in_ (sklearn 1.0+)\n",
    "        if hasattr(model_instance, 'feature_names_in_'):\n",
    "            EXPECTED_COLUMNS = list(model_instance.feature_names_in_)\n",
    "        \n",
    "        # Strategy 2: Check pipeline steps\n",
    "        elif hasattr(model_instance, 'steps'):\n",
    "            for step_name, step_transformer in model_instance.steps:\n",
    "                if hasattr(step_transformer, 'feature_names_in_'):\n",
    "                    EXPECTED_COLUMNS = list(step_transformer.feature_names_in_)\n",
    "                    break\n",
    "                \n",
    "                # Check for ColumnTransformer in pipeline\n",
    "                if hasattr(step_transformer, 'transformers'):\n",
    "                    if hasattr(step_transformer, 'feature_names_in_'):\n",
    "                        EXPECTED_COLUMNS = list(step_transformer.feature_names_in_)\n",
    "                        break\n",
    "        \n",
    "        # Strategy 3: For direct ColumnTransformer, reconstruct from transformers\n",
    "        elif hasattr(model_instance, 'transformers'):\n",
    "            if hasattr(model_instance, 'feature_names_in_'):\n",
    "                EXPECTED_COLUMNS = list(model_instance.feature_names_in_)\n",
    "        \n",
    "        if EXPECTED_COLUMNS is None:\n",
    "            raise ValueError(\"Could not extract column names from model\")\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {len(EXPECTED_COLUMNS)} expected columns from model:\")\n",
    "        print(f\"   {EXPECTED_COLUMNS}\")\n",
    "        \n",
    "        # Create intelligent defaults based on column names\n",
    "        DEFAULT_VALUES = create_intelligent_defaults(EXPECTED_COLUMNS)\n",
    "        \n",
    "        return EXPECTED_COLUMNS, DEFAULT_VALUES\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting model columns: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_intelligent_defaults(columns: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Create intelligent default values based on column names and common patterns.\n",
    "    \"\"\"\n",
    "    defaults = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # Categorical/ID columns\n",
    "        if any(keyword in col_lower for keyword in ['id', 'vendor', 'ratecode', 'payment', 'trip_type']):\n",
    "            defaults[col] = 1\n",
    "        \n",
    "        # Boolean/flag columns\n",
    "        elif any(keyword in col_lower for keyword in ['flag', 'store']):\n",
    "            defaults[col] = 'N'\n",
    "        \n",
    "        # Count columns\n",
    "        elif 'count' in col_lower:\n",
    "            defaults[col] = 1\n",
    "        \n",
    "        # Amount/price/fee/tax columns\n",
    "        elif any(keyword in col_lower for keyword in ['amount', 'fare', 'fee', 'tax', 'tip', 'toll', 'surcharge', 'extra']):\n",
    "            defaults[col] = 0.0\n",
    "        \n",
    "        # Distance columns\n",
    "        elif 'distance' in col_lower:\n",
    "            defaults[col] = 1.0  # Default to 1 mile\n",
    "        \n",
    "        # Duration columns\n",
    "        elif 'duration' in col_lower:\n",
    "            defaults[col] = 10.0  # Default to 10 minutes\n",
    "        \n",
    "        # Location/coordinate columns\n",
    "        elif any(keyword in col_lower for keyword in ['location', 'pu_do', 'pulocation', 'dolocation']):\n",
    "            if col_lower == 'pu_do':\n",
    "                defaults[col] = '1_1'  # Default pickup_dropoff combination\n",
    "            else:\n",
    "                defaults[col] = '1'  # Default location ID\n",
    "        \n",
    "        # Datetime columns (these should be handled separately in your data prep)\n",
    "        elif any(keyword in col_lower for keyword in ['datetime', 'pickup', 'dropoff']):\n",
    "            defaults[col] = None  # Will be handled by your datetime processing\n",
    "        \n",
    "        # Default for unknown columns\n",
    "        else:\n",
    "            # Try to infer from column name patterns\n",
    "            if col_lower.endswith('_id') or col_lower.startswith('id'):\n",
    "                defaults[col] = 1\n",
    "            elif any(char.isalpha() for char in col):  # Contains letters, likely categorical\n",
    "                defaults[col] = 'unknown'\n",
    "            else:  # Likely numeric\n",
    "                defaults[col] = 0.0\n",
    "    \n",
    "    print(f\"üìù Created intelligent defaults:\")\n",
    "    for col, default in defaults.items():\n",
    "        print(f\"   {col}: {default}\")\n",
    "    \n",
    "    return defaults\n",
    "\n",
    "def create_full_feature_dict(feature: Dict, expected_columns: List[str], default_values: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a complete feature dictionary with all expected columns.\n",
    "    Fill missing columns with intelligent default values.\n",
    "    \"\"\"\n",
    "    full_feature = {}\n",
    "    \n",
    "    # Start with defaults for all expected columns\n",
    "    for col in expected_columns:\n",
    "        if col in default_values:\n",
    "            full_feature[col] = default_values[col]\n",
    "        else:\n",
    "            # Fallback default if not in our intelligent defaults\n",
    "            full_feature[col] = 0\n",
    "    \n",
    "    # Override with actual values from input\n",
    "    for key, value in feature.items():\n",
    "        if key in expected_columns:\n",
    "            # Use the provided value, but handle None/empty cases\n",
    "            if value is not None and value != '':\n",
    "                full_feature[key] = value\n",
    "            # If value is None/empty, keep the default\n",
    "    \n",
    "    return full_feature\n",
    "\n",
    "def dict_to_dataframe_compatible_array(features_list: List[Dict], expected_columns: List[str], default_values: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert list of feature dictionaries to a 2D numpy array compatible with sklearn ColumnTransformer.\n",
    "    Returns array with shape (n_samples, n_features) where columns are in expected_columns order.\n",
    "    \"\"\"\n",
    "    if not features_list:\n",
    "        return np.array([]).reshape(0, len(expected_columns))\n",
    "    \n",
    "    # Ensure all features have the same structure\n",
    "    structured_features = [create_full_feature_dict(f, expected_columns, default_values) for f in features_list]\n",
    "    \n",
    "    # Create 2D array with proper column ordering\n",
    "    n_samples = len(structured_features)\n",
    "    n_features = len(expected_columns)\n",
    "    \n",
    "    # Initialize array - we'll use object dtype to handle mixed types\n",
    "    result_array = np.empty((n_samples, n_features), dtype=object)\n",
    "    \n",
    "    for i, feature in enumerate(structured_features):\n",
    "        for j, col in enumerate(expected_columns):\n",
    "            value = feature[col]\n",
    "            if value is None:\n",
    "                # Handle None values based on column type patterns\n",
    "                if any(keyword in col.lower() for keyword in ['flag', 'store']):\n",
    "                    result_array[i, j] = 'N'\n",
    "                elif any(keyword in col.lower() for keyword in ['amount', 'fare', 'fee', 'tax', 'tip', 'toll', 'distance', 'duration']):\n",
    "                    result_array[i, j] = 0.0\n",
    "                elif any(keyword in col.lower() for keyword in ['id', 'vendor', 'ratecode', 'payment', 'trip_type', 'count']):\n",
    "                    result_array[i, j] = 1\n",
    "                else:\n",
    "                    result_array[i, j] = 'unknown'\n",
    "            else:\n",
    "                result_array[i, j] = value\n",
    "    \n",
    "    print(f\"üìä Created 2D array with shape: {result_array.shape}\")\n",
    "    print(f\"   Columns (first 5): {expected_columns[:5]}\")\n",
    "    print(f\"   Sample row: {result_array[0] if len(result_array) > 0 else 'No data'}\")\n",
    "    \n",
    "    return result_array\n",
    "\n",
    "def make_prediction_batch(features: List[Dict]) -> List[float]:\n",
    "    \"\"\"Make predictions for a batch of features using 2D array compatible with ColumnTransformer\"\"\"\n",
    "    try:\n",
    "        model_instance = model2\n",
    "        \n",
    "        # Extract expected columns and defaults from model on first run\n",
    "        expected_columns, default_values = extract_model_columns_and_defaults(model_instance)\n",
    "        \n",
    "        # Convert to 2D array that's compatible with ColumnTransformer\n",
    "        prediction_data = dict_to_dataframe_compatible_array(features, expected_columns, default_values)\n",
    "        \n",
    "        print(f\"üìä Prepared prediction data:\")\n",
    "        print(f\"   Shape: {prediction_data.shape}\")\n",
    "        print(f\"   Expected columns: {len(expected_columns)}\")\n",
    "        print(f\"   Provided features: {len(features)}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = model_instance.predict(prediction_data)\n",
    "        \n",
    "        print(f\"‚úÖ Predictions successful!\")\n",
    "        print(f\"   Predictions shape: {predictions.shape if hasattr(predictions, 'shape') else len(predictions)}\")\n",
    "        \n",
    "        # Ensure predictions is a list\n",
    "        if hasattr(predictions, 'tolist'):\n",
    "            return predictions.tolist()\n",
    "        elif isinstance(predictions, (list, tuple)):\n",
    "            return list(predictions)\n",
    "        else:\n",
    "            return [float(predictions)]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction error: {str(e)}\")\n",
    "        print(f\"Prediction data shape: {prediction_data.shape if 'prediction_data' in locals() else 'Not created'}\")\n",
    "        print(f\"Available columns in features: {list(features[0].keys()) if features else 'No features'}\")\n",
    "        raise\n",
    "\n",
    "# Alternative approach using pandas DataFrame (if you have pandas available)\n",
    "def make_prediction_batch_with_pandas(features: List[Dict]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Alternative implementation using pandas DataFrame for maximum compatibility.\n",
    "    Use this if you have pandas available and want the most reliable approach.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    try:\n",
    "        model_instance = model2\n",
    "        \n",
    "        # Extract expected columns and defaults from model on first run\n",
    "        expected_columns, default_values = extract_model_columns_and_defaults(model_instance)\n",
    "        \n",
    "        # Create list of complete feature dictionaries\n",
    "        complete_features = [create_full_feature_dict(f, expected_columns, default_values) for f in features]\n",
    "        \n",
    "        # Convert to DataFrame with proper column order\n",
    "        df = pd.DataFrame(complete_features, columns=expected_columns)\n",
    "        \n",
    "        print(f\"üìä Created DataFrame:\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        print(f\"   Sample row:\\n{df.iloc[0] if len(df) > 0 else 'No data'}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = model_instance.predict(df)\n",
    "        \n",
    "        print(f\"‚úÖ Predictions successful!\")\n",
    "        \n",
    "        # Ensure predictions is a list\n",
    "        if hasattr(predictions, 'tolist'):\n",
    "            return predictions.tolist()\n",
    "        elif isinstance(predictions, (list, tuple)):\n",
    "            return list(predictions)\n",
    "        else:\n",
    "            return [float(predictions)]\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_data = {\n",
    "    \"data\": [{\n",
    "        \"PULocationID\": \"100\", \n",
    "        \"DOLocationID\": \"200\",\n",
    "        \"trip_distance\": 2.5,\n",
    "        \"lpep_pickup_datetime\": \"2024-01-01 10:00:00\",\n",
    "        \"lpep_dropoff_datetime\": \"2024-01-01 10:15:00\"\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 22 expected columns from model:\n",
      "   ['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge', 'duration', 'PU_DO']\n",
      "üìù Created intelligent defaults:\n",
      "   VendorID: 1\n",
      "   lpep_pickup_datetime: None\n",
      "   lpep_dropoff_datetime: None\n",
      "   store_and_fwd_flag: N\n",
      "   RatecodeID: 1\n",
      "   PULocationID: 1\n",
      "   DOLocationID: 1\n",
      "   passenger_count: 1\n",
      "   trip_distance: 1.0\n",
      "   fare_amount: 0.0\n",
      "   extra: 0.0\n",
      "   mta_tax: 0.0\n",
      "   tip_amount: 0.0\n",
      "   tolls_amount: 0.0\n",
      "   ehail_fee: 0.0\n",
      "   improvement_surcharge: 0.0\n",
      "   total_amount: 0.0\n",
      "   payment_type: 1\n",
      "   trip_type: 1\n",
      "   congestion_surcharge: 0.0\n",
      "   duration: 10.0\n",
      "   PU_DO: 1_1\n",
      "üìä Created 2D array with shape: (1, 22)\n",
      "   Columns (first 5): ['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID']\n",
      "   Sample row: [1 '2024-01-01 10:00:00' '2024-01-01 10:15:00' 'N' 1 '100' '200' 1 2.5 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1 0.0 10.0 '1_1']\n",
      "üìä Prepared prediction data:\n",
      "   Shape: (1, 22)\n",
      "   Expected columns: 22\n",
      "   Provided features: 1\n",
      "‚ùå Prediction error: Specifying the columns using strings is only supported for dataframes.\n",
      "Prediction data shape: (1, 22)\n",
      "Available columns in features: ['PULocationID', 'DOLocationID', 'trip_distance', 'lpep_pickup_datetime', 'lpep_dropoff_datetime']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for dataframes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m make_prediction_batch(minimal_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[26], line 194\u001b[0m, in \u001b[0;36mmake_prediction_batch\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Provided features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mpredict(prediction_data)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Predictions successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Predictions shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mhasattr\u001b[39m(predictions,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/pipeline.py:600\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:1076\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1074\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[0;32m-> 1076\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_func_on_transformers(\n\u001b[1;32m   1077\u001b[0m     X,\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1079\u001b[0m     _transform_one,\n\u001b[1;32m   1080\u001b[0m     column_as_labels\u001b[38;5;241m=\u001b[39mfit_dataframe_and_transform_dataframe,\n\u001b[1;32m   1081\u001b[0m     routed_params\u001b[38;5;241m=\u001b[39mrouted_params,\n\u001b[1;32m   1082\u001b[0m )\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:877\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# func is _transform_one\u001b[39;00m\n\u001b[1;32m    873\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    874\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    875\u001b[0m             delayed(func)(\n\u001b[1;32m    876\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m--> 877\u001b[0m                 X\u001b[38;5;241m=\u001b[39m_safe_indexing(X, columns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    878\u001b[0m                 y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    879\u001b[0m                 weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    880\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args,\n\u001b[1;32m    881\u001b[0m                 params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    882\u001b[0m             )\n\u001b[1;32m    883\u001b[0m         )\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(jobs)\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/serverlessML/lib/python3.12/site-packages/sklearn/utils/_indexing.py:256\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be a 2D NumPy array, 2D sparse matrix or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe when indexing the columns (i.e. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis=1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead with \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimension(s).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(X), \u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    252\u001b[0m     axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m indices_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (_is_pandas_df(X) \u001b[38;5;129;01mor\u001b[39;00m _use_interchange_protocol(X))\n\u001b[1;32m    255\u001b[0m ):\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# TODO: we should probably use _is_pandas_df_or_series(X) instead but this\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# would require updating some tests such as test_train_test_split_mock_pandas.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for dataframes."
     ]
    }
   ],
   "source": [
    "make_prediction_batch(minimal_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serverlessML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
